---
title: "[ NLP Study ] 0807 - RoBERTa, BART and T5" 
date: 2021-08-07 23:57:30 -0400
categories: til Transformers nlp\ 대충\ 스터디
---

자연어처리에서, transformer 기반 모델의 발전 과정 중 흥미로운 논문들을 골랐습니다

더 예쁘게, 사진과 함께 보시려면 아래 링크로...

https://sepia-slash-f97.notion.site/0814-290d32a43e0146979a10777950e2b767


**Transformers**
기계 번역을 위해 제안된 네트워크. 텍스트와 같은 sequential 데이터에서 요소들 사이의 관계를 파악하는 self-attention으로 구성된 encoder와 decoder 구조를 갖고 있다


## RoBERTa

> RoBERTa: A Robustly Optimized BERT Pretraining Approach
*Liu et al.,  2019*

BERT를 최적화 해보자!

BERT가 underfitting 되었다는 증거 포착, BERT 최적화로 GLUE, RACE, SQuAD 등의 벤치마크에서 SOTA 기록

BERT에 최적화를 적용해 모델의 학습 시간을 증가시키고, batch size와 train data 또한 증가시켜 성능을 개선했다
( 모델의 구조에는 변화가 없으나, 성능을 크게 향상시킴 )

Next sentence prediction 제거 및 더 긴 sequence 추가, masking 패턴을 동적으로 변경

**모델 구조: BERT와 동일**

- **학습 과정 분석, 방법 제시**
    - **1️⃣  Dynamic Masking** 방법 제안

        기존 BERT의 경우 사전학습 전 미리 마스킹을 진행하기 때문에, 모델이 학습할 때 똑같은 토큰이 마스킹 되어 편향이 발생할 것으로 추정
        → 동일한 데이터에 마스킹을 10번 다르게 적용, 인풋이 들어갈 때마다 마스킹
        ⇒ 성능 사알짝 개선되거나/유사한 결과

    - 2️⃣  **모델 인풋 형식, Next Sentence Prediction** 제거

        각 문장들을 사용하는 것이 downstream task에서 성능 저하를 일으켰는데, 모델이 더 긴 범위의 의존 관계를 학습할 수 없기 때문이라고 분석함

        ⇒ NSP 손실 함수를 제거하는 것이 성능을 약간 향상시킴을 보임

    - 3️⃣  **큰 batch 크기로 학습**
    - 4️⃣  **텍스트 인코딩 방식 - BPE with larger byte-level**

        기존 BERT에서는 입력을 전처리하고 토큰화

        ⇒ 추가적인 전처리나 토큰화를 사용하지 않고, 더 큰 byte-level의 BPE vocabulary로 학습

- **결과**

    **논문 공개 당시 SOTA 달성**

    1. GLUE 리더보드 결과


    2. SQuAD, RACE 결과


## BART

> BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension
*Lewis et al., 2019*

Denoising autoencoder 학습
다양한 noising 방법 추가 → 텍스트 생성에서 SOTA 달성

BART는 **denoising autoencoder**로, 다양한 transformation 방식으로 document를 훼손한 후 이를 복원하는 방식으로 pretraining 되었다

**Transformation** 방식들 (noising) → Pretrian에 사용
- 변형된 text infilling
- Sentence permutation
- Document rotation

**Finetuning** 방식을 새롭게 제안했다
- 텍스트 분류를 위해 decoder의 마지막 토큰 입력 사용
- 기계 번역을 위해 인코더 학습

- **모델 구조  $\approx$  BERT + GPT**

    Seq2Seq 트랜스포머 구조


- **Pretraining 방법 제안, BERT와 비교**

    Noise input → Original text 복원하는 방식으로 모델 학습

    *Pretraining을 위한 transformation 방법들 (noise 생성)*

    1. Token Masking & Deletion

        랜덤한 토큰을 추출해 마스킹 하거나, 삭제한다

    2. **Text Infilling**

        여러 개의 span을 샘플링하고, 0개 ~ 여러 개를 포함할 수 있는 span을 하나의 [MASK] 토큰으로 대체함
        ⇒ 모델이 span 내 몇 개의 토큰이 사라졌는지도 예측하도록 함

    3. **Sentence Permutation**
    문장들을 랜덤한 순서로 섞음
    4. **Document Rotation**

        토큰을 랜덤하게 선정하고 그 뒤 부분으로 document를 회전해, 선정된 토큰부터 문서가 시작되도록 한다

        ⇒ 모델이 문서 시작점을 판별할 수 있도록 학습됨

    *BERT와 pretraining objective 비교 분석*

    - 토큰 마스킹은 필수
    - Left-to-right 사전학습이 텍스트 생성 성능을 높임
    - SQuAD에는 양방향 인코더 필수

- **Finetuning 방법**


    1. **Sequence Classification Tasks**

        인코더와 디코더에 같은 입력이 주어지며, 디코더의 final hidden state가 새로운 linear classifier로 전달됨

        [CLS]토큰과 유사하지만 마지막 토큰까지 입력해주어서 전체 입력 문장에 대한 디코더의 attention을 계산함

    2. **Token Classification Tasks**

        전체 문서를 인코더와 디코더에 입력, 디코더의 top hidden state를 각 단어에 대한 representation으로 사용해 classification 수행

    3. **Sequence Generation Tasks**

        문장 생성과 동일하게 학습 가능. Autoregressive decoder를 갖고 있기 때문에 바로 파인튜닝이 가능함

    4. **Machine Translation**
        - 전체 BART모델을 기계 번역을 위한 사전학습된 "decoder"로 사용
        - 새로운 인코더 추가 학습  ⇒ 인코더(new) - 디코더 (BART) 파인튜닝
            - Transformer 레이어들
            - 외국어를 BART가 학습한 언어 (영어)로 번역하는 역할 수행, 노이즈가 있음 ⇒ BART를 거쳐 완성
        - 학습 방법 두 단계:
            1. BART 파라미터 대부분을 픽스, 새 인코더와 BART의 position embedding, BART 인코더의 첫 번째 레이어의 self-attention input projection matrix만 학습
            2. 이후, 모든 파라미터 학습
- **성능: 논문 공개 당시 SOTA 달성**


## ~~T5~~

> Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (T5)
*Raffel et al.,  JMLR, 2020*

모든 NLP 태스크를 text-to-text 문제로 통합 해보자!

Text-to-Text 프레임워크로 자연어 처리의 다양한 태스크를 공통된 text-to-text 형식으로 통합해 SOTA 성능을 기록했다
* Text-to-text란?: 텍스트 형태로 주어진 문제에서 텍스트 형태의 정답을 찾는 문제

Basic transformer구조를 사용하고, 인코더-디코더 구조로 모든 NLP 태스크를 multi-task learning으로 학습해 모든 태스크에서 동일한 모델, 손실함수, 하이퍼파라미터를 적용할 수 있다.

Pretraining을 위한 대규모 데이터셋, Colossal Clean  Crawled Corpus (C4)를 제작했다

- **모델 구조  (기본 transformer 변형)**

    대부분 basic transformer 구조 사용함


    - Encoder-decoder 구조

        인코더에 입력 문장 → 디코더가 새로운 문장 생성

    - Language model

        Autoregressive하게 출력 문장 생성

    - Prefix language model

- **Text-to-text Framework**


    모든 텍스트 기반 태스크들을 text-to-text 형식으로 변환하고, Multitask learning으로 모델을 학습시켰다

    Cola는 문법을 확인하는 데이터셋. classification 태스크를 text-to-text로 변경하는 예시가 위 그림에 나와있음. 원래는 "The course is jumping well" 라벨: 0 이었다면 → "cola sentence: The course is jumping well" 라벨: "not acceptable" 이런 식으로 변환

    ⇒ 모든 태스크에 대해 동일한 모델, 손실함수, 하이퍼 파라미터 등을 사용할 수 있음

- **C4 Dataset**

    라벨링 되지 않은 대규모 데이터셋 공개

- **결과**

    GLUE, SuperGLUE, CNN/DM 및 SQuAD 벤치마크에서 SOTA 결과 달성
